{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Main Idea of word2vec\n",
    "- Iterate through each word of the whole corpus\n",
    "- predict surrounding words using word vectors\n",
    "- Update word vectors so you can predict well\n",
    "\n",
    "Note: this model will output the same predictions for each position. I.e. it wil say that \"house\" has a prob of X% to be the next word, and X% to be th eprevious word, and X% to be the second next word, etc.\n",
    "\n",
    "So a good model is one that gives a high probability to ALL words that occur in that context.\n",
    "\n",
    "It does this by putting similar words nearby in space\n",
    "\n",
    "This is the **Skip Gram** Method. You predict context words using the center word\n",
    "You can also use **Bag of Words**. where you predict the center word given the context words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Why not capture co-occurence counts directly?\n",
    "\n",
    "- With a co-occurence matrix X\n",
    "    - 2 options: windows vs full document\n",
    "- Window:\n",
    "    - similar to word2vec\n",
    "    - use window around each word (token), just count up how often words occur together\n",
    "    - and create an nxn matrix from it (n=# of tokens). Similar to a correlation matrix \n",
    "    - captures both syntactic and semantic information\n",
    "    - Problems:\n",
    "        - increase in size with vocaulary. very high dimensional -> sparsity problems.\n",
    "        - less robust\n",
    "    - Solution: low dimensional vectors:\n",
    "        - reduce dimensionality using SVD <img src = 'Pictures/SVD.png' width = '400'>\n",
    "    - Works ok, but never worked well (and terribly on analogies)\n",
    "    - Tweaks:\n",
    "        - Scaling the counts (or simply cieling/ignore high frequency counts)\n",
    "        - Ramped windows that give higher weight when words are closer\n",
    "        - use Pearson correlations instead of counts (negatives set to 0)\n",
    "        - With these tweaks it's not bad at analogies\n",
    "\n",
    "## Count based vs Direct Prediction\n",
    "\n",
    "- Count:\n",
    "    - LSA, HAL, COALS, \n",
    "    - Fast training, efficient usage of statistics\n",
    "    - Primarily used for word similarity, disprorpotionate importance to large counts\n",
    "- Direct Prediction\n",
    "    - Skip-Gram, RNN\n",
    "    - Scales with corpus size, inefficient usage of statistics\n",
    "    - Better performance on other tasks, captures complex patterns BEYOND word similarity\n",
    "\n",
    "## Crucial Insight:  Ratios of co-occurence probabilities can encode meaning components\n",
    "<img src = 'Pictures/cooccurence.png' width = '400'>  \\\\n\n",
    "\n",
    "<img src = 'Pictures/logcooc.png' width = '400'>\n",
    "\n",
    "# the GloVe model\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "GloVe analogy evaluation\n",
    "- Good dimensionality is ~300.\n",
    "- Assymmetric context (only words on left) are not as good (assuming window > 2)\n",
    "- window size of 8 around each center word is good for Glove vectors\n",
    "\n",
    "Studies have shown that as you increase dimensionality to a huge parameter, the performance pretty much stays flat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Word senses and ambiguity\n",
    "\n",
    "Most words have lots of meanings, especially common words and old words\n",
    "\n",
    "Does one vector capture all these??"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Idea: \n",
    "- for each common word, cluster them in the context they appear\n",
    "- and then treat each \"cluster\" word differently (each have their own vector). i.e. bank1, bank2, bank3\n",
    "\n",
    "but pretty crude, \n",
    "\n",
    "Idea 2: \n",
    "- different senses of a word, are the weighted sum (superposition) of the word vector of the underlying word senses (weighted by their frequency)\n",
    "- now you might think you cant back-out the underlying parts from the overall superposition\n",
    "- but in reality, the spaces are so sparse, you CAN separate out senses (assuming they are common enough)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}